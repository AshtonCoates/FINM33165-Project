{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collector for Earnings Event-Driven RL Trading\n",
    "\n",
    "This notebook processes raw price data and earnings dates into a structured dataset ready for environment building.\n",
    "\n",
    "## Overview\n",
    "1. Load raw price and earnings data\n",
    "2. Build event windows (N days before and after earnings)\n",
    "3. Calculate technical features\n",
    "4. Generate training/test splits\n",
    "5. Export to CSV format for environment building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Data Paths ============\n",
    "RAW_PRICE_DATA = \"MAG7_hourly_5yr.csv\"  # Change this to your file path\n",
    "EARNINGS_DATA = \"mag7_earnings_dates_2019_2024.csv\"  # Change this to your file path\n",
    "OUTPUT_CSV = \"earnings_events_data.csv\"  # Output file\n",
    "\n",
    "# ============ Event Window Configuration ============\n",
    "DAYS_BEFORE = 5      # Days before earnings\n",
    "DAYS_AFTER = 5       # Days after earnings\n",
    "TOTAL_WINDOW = DAYS_BEFORE + DAYS_AFTER + 1  # Total days = 11\n",
    "\n",
    "# ============ Feature Calculation ============\n",
    "MOMENTUM_WINDOW = 3   # Calculate momentum using past 3 days\n",
    "VOLATILITY_WINDOW = 3  # Calculate volatility using past 3 days\n",
    "\n",
    "# ============ Trading Costs ============\n",
    "TRANSACTION_COST = 0.0005  # 0.05% (round-trip)\n",
    "\n",
    "# ============ Train/Test Split ============\n",
    "TRAIN_TEST_SPLIT = 0.80  # 80% training, 20% test\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Price Data: {RAW_PRICE_DATA}\")\n",
    "print(f\"  Earnings Data: {EARNINGS_DATA}\")\n",
    "print(f\"  Output CSV: {OUTPUT_CSV}\")\n",
    "print(f\"  Event Window: {DAYS_BEFORE} days before + {DAYS_AFTER} days after\")\n",
    "print(f\"  Train/Test Split: {TRAIN_TEST_SPLIT:.0%}/{1-TRAIN_TEST_SPLIT:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading raw data...\")\n",
    "\n",
    "# Load price data\n",
    "price_data = pd.read_csv(RAW_PRICE_DATA)\n",
    "price_data['timestamp'] = pd.to_datetime(price_data['timestamp'])\n",
    "price_data = price_data.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Load earnings dates\n",
    "earnings_dates = pd.read_csv(EARNINGS_DATA)\n",
    "earnings_dates['earnings_date'] = pd.to_datetime(earnings_dates['earnings_date'])\n",
    "\n",
    "logger.info(f\"âœ“ Loaded {len(price_data):,} price records\")\n",
    "logger.info(f\"âœ“ Loaded {len(earnings_dates)} earnings release dates\")\n",
    "\n",
    "print(\"\\nPrice Data Sample:\")\n",
    "print(price_data.head())\n",
    "print(f\"\\nPrice Data Info:\")\n",
    "print(f\"  - Date Range: {price_data['timestamp'].min()} to {price_data['timestamp'].max()}\")\n",
    "print(f\"  - Tickers: {price_data['ticker'].unique().tolist()}\")\n",
    "print(f\"  - Total Records: {len(price_data):,}\")\n",
    "\n",
    "print(\"\\nEarnings Data Sample:\")\n",
    "print(earnings_dates.head())\n",
    "print(f\"\\nEarnings Data Info:\")\n",
    "print(f\"  - Date Range: {earnings_dates['earnings_date'].min()} to {earnings_dates['earnings_date'].max()}\")\n",
    "print(f\"  - Total Dates: {len(earnings_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Event Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Building event windows...\")\n",
    "\n",
    "all_events_data = []\n",
    "valid_events_count = 0\n",
    "skipped_events_count = 0\n",
    "\n",
    "for _, earnings_row in earnings_dates.iterrows():\n",
    "    ticker = earnings_row['ticker']\n",
    "    earnings_date = pd.Timestamp(earnings_row['earnings_date'])\n",
    "    \n",
    "    # Get price data for this company\n",
    "    ticker_data = price_data[price_data['ticker'] == ticker].copy()\n",
    "    \n",
    "    # Define event window\n",
    "    window_start = earnings_date - timedelta(days=DAYS_BEFORE)\n",
    "    window_end = earnings_date + timedelta(days=DAYS_AFTER)\n",
    "    \n",
    "    # Extract window data - convert to date for comparison (avoid timezone issues)\n",
    "    ticker_data['timestamp_date'] = ticker_data['timestamp'].dt.date\n",
    "    earnings_start_date = window_start.date()\n",
    "    earnings_end_date = window_end.date()\n",
    "    \n",
    "    window_data = ticker_data[\n",
    "        (ticker_data['timestamp_date'] >= earnings_start_date) & \n",
    "        (ticker_data['timestamp_date'] <= earnings_end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # Check data completeness\n",
    "    if len(window_data) < 50:  # Approximately 5 days * 10 hours\n",
    "        skipped_events_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    window_data = window_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    window_data = window_data.drop('timestamp_date', axis=1)  # Remove helper column\n",
    "    \n",
    "    # Add event metadata\n",
    "    window_data['ticker_event'] = ticker\n",
    "    window_data['earnings_date'] = earnings_date\n",
    "    window_data['window_start'] = window_start\n",
    "    window_data['window_end'] = window_end\n",
    "    window_data['event_id'] = valid_events_count\n",
    "    \n",
    "    all_events_data.append(window_data)\n",
    "    valid_events_count += 1\n",
    "\n",
    "logger.info(f\"âœ“ Built {valid_events_count} valid event windows\")\n",
    "logger.info(f\"âŠ˜ Skipped {skipped_events_count} events due to insufficient data\")\n",
    "\n",
    "print(f\"\\nEvent Windows Summary:\")\n",
    "print(f\"  - Valid Events: {valid_events_count}\")\n",
    "print(f\"  - Skipped Events: {skipped_events_count}\")\n",
    "print(f\"  - Total Data Points in Events: {sum(len(e) for e in all_events_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Technical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(window_data, earnings_date):\n",
    "    \"\"\"\n",
    "    Calculate technical features for an event window.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - momentum: Pre-earnings momentum\n",
    "    - volatility: Pre-earnings volatility\n",
    "    - avg_volume: Average volume\n",
    "    - pre_close: Pre-earnings close price\n",
    "    - window_return: Window return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert earnings_date to date for comparison\n",
    "        if hasattr(earnings_date, 'date'):\n",
    "            earnings_date_compare = earnings_date.date()\n",
    "        else:\n",
    "            earnings_date_compare = earnings_date\n",
    "        \n",
    "        # Separate pre-earnings data\n",
    "        if 'timestamp_date' in window_data.columns:\n",
    "            pre_earnings = window_data[window_data['timestamp_date'] < earnings_date_compare]\n",
    "        else:\n",
    "            window_data_copy = window_data.copy()\n",
    "            window_data_copy['temp_date'] = window_data_copy['timestamp'].dt.date\n",
    "            pre_earnings = window_data_copy[window_data_copy['temp_date'] < earnings_date_compare]\n",
    "            pre_earnings = pre_earnings.drop('temp_date', axis=1)\n",
    "        \n",
    "        if len(pre_earnings) < 20:\n",
    "            return None\n",
    "        \n",
    "        # 1. Pre-earnings momentum (return over past N days)\n",
    "        pre_returns = pre_earnings['close'].pct_change().dropna()\n",
    "        momentum = pre_returns.tail(MOMENTUM_WINDOW).sum()\n",
    "        \n",
    "        # 2. Volatility (std dev over past N days)\n",
    "        volatility = pre_returns.tail(VOLATILITY_WINDOW).std()\n",
    "        \n",
    "        # 3. Average volume\n",
    "        avg_volume = pre_earnings['volume'].tail(VOLATILITY_WINDOW).mean()\n",
    "        \n",
    "        # 4. Pre-earnings closing price\n",
    "        pre_close = pre_earnings['close'].iloc[-1]\n",
    "        \n",
    "        # 5. Window return (full window)\n",
    "        pre_price = pre_earnings['close'].iloc[0]\n",
    "        \n",
    "        if 'timestamp_date' in window_data.columns:\n",
    "            post_earnings = window_data[window_data['timestamp_date'] > earnings_date_compare]\n",
    "        else:\n",
    "            window_data_copy = window_data.copy()\n",
    "            window_data_copy['temp_date'] = window_data_copy['timestamp'].dt.date\n",
    "            post_earnings = window_data_copy[window_data_copy['temp_date'] > earnings_date_compare]\n",
    "            post_earnings = post_earnings.drop('temp_date', axis=1)\n",
    "        \n",
    "        if len(post_earnings) > 0:\n",
    "            post_price = post_earnings['close'].iloc[-1]\n",
    "            window_return = (post_price - pre_price) / pre_price\n",
    "        else:\n",
    "            window_return = 0\n",
    "        \n",
    "        return {\n",
    "            'momentum': momentum,\n",
    "            'volatility': volatility if volatility > 0 else 0.01,\n",
    "            'avg_volume': avg_volume,\n",
    "            'pre_close': pre_close,\n",
    "            'window_return': window_return\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Feature calculation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "logger.info(\"Calculating features for all events...\")\n",
    "\n",
    "# Add features to each event\n",
    "processed_events = []\n",
    "for event_data in all_events_data:\n",
    "    ticker = event_data['ticker_event'].iloc[0]\n",
    "    earnings_date = event_data['earnings_date'].iloc[0]\n",
    "    \n",
    "    features = calculate_features(event_data, earnings_date)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Add features to each row in the event\n",
    "        event_data['momentum'] = features['momentum']\n",
    "        event_data['volatility'] = features['volatility']\n",
    "        event_data['avg_volume_event'] = features['avg_volume']\n",
    "        event_data['pre_close'] = features['pre_close']\n",
    "        event_data['window_return'] = features['window_return']\n",
    "        \n",
    "        processed_events.append(event_data)\n",
    "\n",
    "logger.info(f\"âœ“ Calculated features for {len(processed_events)} events\")\n",
    "\n",
    "print(f\"\\nFeature Calculation Summary:\")\n",
    "print(f\"  - Events with Features: {len(processed_events)}\")\n",
    "print(f\"  - Total Data Points: {sum(len(e) for e in processed_events):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all events into single dataframe\n",
    "combined_data = pd.concat(processed_events, ignore_index=True)\n",
    "\n",
    "# Get unique events for train/test split\n",
    "unique_events = combined_data[['event_id', 'earnings_date']].drop_duplicates('event_id')\n",
    "\n",
    "# Split events (not individual rows)\n",
    "train_size = int(len(unique_events) * TRAIN_TEST_SPLIT)\n",
    "train_event_ids = unique_events.iloc[:train_size]['event_id'].values\n",
    "test_event_ids = unique_events.iloc[train_size:]['event_id'].values\n",
    "\n",
    "# Add split label\n",
    "combined_data['split'] = combined_data['event_id'].apply(\n",
    "    lambda x: 'train' if x in train_event_ids else 'test'\n",
    ")\n",
    "\n",
    "logger.info(f\"âœ“ Train/Test Split:\")\n",
    "logger.info(f\"  - Training events: {len(train_event_ids)}\")\n",
    "logger.info(f\"  - Test events: {len(test_event_ids)}\")\n",
    "\n",
    "print(f\"\\nTrain/Test Split Summary:\")\n",
    "print(f\"  - Training Events: {len(train_event_ids)}\")\n",
    "print(f\"  - Test Events: {len(test_event_ids)}\")\n",
    "print(f\"  - Training Rows: {len(combined_data[combined_data['split'] == 'train']):,}\")\n",
    "print(f\"  - Test Rows: {len(combined_data[combined_data['split'] == 'test']):,}\")\n",
    "\n",
    "print(f\"\\nData Sample:\")\n",
    "print(combined_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique features per event\n",
    "event_features = combined_data.groupby('event_id').agg({\n",
    "    'momentum': 'first',\n",
    "    'volatility': 'first',\n",
    "    'avg_volume_event': 'first',\n",
    "    'window_return': 'first',\n",
    "    'split': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\n=== Feature Statistics ===\")\n",
    "print(\"\\nTraining Set Features:\")\n",
    "train_features = event_features[event_features['split'] == 'train']\n",
    "print(train_features[['momentum', 'volatility', 'window_return']].describe())\n",
    "\n",
    "print(\"\\nTest Set Features:\")\n",
    "test_features = event_features[event_features['split'] == 'test']\n",
    "print(test_features[['momentum', 'volatility', 'window_return']].describe())\n",
    "\n",
    "print(f\"\\nWindow Return Distribution:\")\n",
    "print(f\"  - Positive Returns: {(event_features['window_return'] > 0).sum()} events\")\n",
    "print(f\"  - Negative Returns: {(event_features['window_return'] < 0).sum()} events\")\n",
    "print(f\"  - Mean Return: {event_features['window_return'].mean():.4f}\")\n",
    "print(f\"  - Median Return: {event_features['window_return'].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare CSV for Environment Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for environment\n",
    "csv_columns = [\n",
    "    'ticker_event',\n",
    "    'earnings_date',\n",
    "    'window_start',\n",
    "    'window_end',\n",
    "    'event_id',\n",
    "    'timestamp',\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'volume',\n",
    "    'momentum',\n",
    "    'volatility',\n",
    "    'avg_volume_event',\n",
    "    'pre_close',\n",
    "    'window_return',\n",
    "    'split'\n",
    "]\n",
    "\n",
    "output_data = combined_data[csv_columns].copy()\n",
    "\n",
    "# Sort by event and timestamp\n",
    "output_data = output_data.sort_values(['event_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_data.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "logger.info(f\"âœ“ Saved CSV to {OUTPUT_CSV}\")\n",
    "\n",
    "print(f\"\\nâœ… CSV Export Complete!\")\n",
    "print(f\"\\nFile: {OUTPUT_CSV}\")\n",
    "print(f\"\\nColumns:\")\n",
    "for i, col in enumerate(csv_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nDataset Size:\")\n",
    "print(f\"  - Total Rows: {len(output_data):,}\")\n",
    "print(f\"  - Total Columns: {len(csv_columns)}\")\n",
    "print(f\"  - File Size: {Path(OUTPUT_CSV).stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nFirst Few Rows:\")\n",
    "print(output_data.head())\n",
    "\n",
    "print(f\"\\nLast Few Rows:\")\n",
    "print(output_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Data Quality Check ===\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = output_data.isnull().sum()\n",
    "print(f\"\\nMissing Values:\")\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    print(f\"  - {col}: {missing_values[col]}\")\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"  âœ“ No missing values\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = output_data.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "# Check split distribution\n",
    "print(f\"\\nSplit Distribution:\")\n",
    "print(output_data['split'].value_counts())\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData Types:\")\n",
    "print(output_data.dtypes)\n",
    "\n",
    "# Check for outliers\n",
    "print(f\"\\nPrice Range Check:\")\n",
    "print(f\"  - Close Price Min: ${output_data['close'].min():.2f}\")\n",
    "print(f\"  - Close Price Max: ${output_data['close'].max():.2f}\")\n",
    "print(f\"  - Volume Min: {output_data['volume'].min():,}\")\n",
    "print(f\"  - Volume Max: {output_data['volume'].max():,}\")\n",
    "\n",
    "print(f\"\\nâœ… Data Quality Check Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Usage Instructions for Environment Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                    CSV Data Ready for Environment Building                       â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ“ Output File: earnings_events_data.csv\n\nğŸ“‹ How to Use in Environment:\n\n1. Load the CSV:\n   ```python\n   import pandas as pd\n   df = pd.read_csv('earnings_events_data.csv')\n   ```\n\n2. Get unique events:\n   ```python\n   events = df.groupby('event_id').first().reset_index()\n   train_events = events[events['split'] == 'train']\n   test_events = events[events['split'] == 'test']\n   ```\n\n3. For each event, create environment:\n   ```python\n   event_data = df[df['event_id'] == event_id]\n   env = EarningsEventEnv({\n       'ticker': event_data['ticker_event'].iloc[0],\n       'earnings_date': event_data['earnings_date'].iloc[0],\n       'price_data': event_data[['timestamp', 'open', 'high', 'low', 'close', 'volume']],\n       'features': {\n           'momentum': event_data['momentum'].iloc[0],\n           'volatility': event_data['volatility'].iloc[0],\n           'avg_volume': event_data['avg_volume_event'].iloc[0],\n           'pre_close': event_data['pre_close'].iloc[0],\n           'window_return': event_data['window_return'].iloc[0]\n       }\n   })\n   ```\n\nğŸ“Š CSV Columns Description:\n   - ticker_event: Stock ticker symbol\n   - earnings_date: Earnings announcement date\n   - window_start: Event window start date\n   - window_end: Event window end date\n   - event_id: Unique event identifier\n   - timestamp: Price timestamp (hourly)\n   - open/high/low/close: OHLC prices\n   - volume: Trading volume\n   - momentum: Pre-earnings momentum\n   - volatility: Pre-earnings volatility\n   - avg_volume_event: Average volume in event window\n   - pre_close: Closing price before earnings\n   - window_return: Return from start to end of window\n   - split: Train/Test split (train/test)\n\nâœ… Ready to build environments!\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"  - Total Events: {output_data['event_id'].nunique()}\")\n",
    "print(f\"  - Training Events: {len(output_data[output_data['split'] == 'train']['event_id'].unique())}\")\n",
    "print(f\"  - Test Events: {len(output_data[output_data['split'] == 'test']['event_id'].unique())}\")\n",
    "print(f\"  - Total Data Points: {len(output_data):,}\")\n",
    "print(f\"  - Unique Tickers: {output_data['ticker_event'].nunique()}\")\n",
    "print(f\"  - Date Range: {output_data['timestamp'].min()} to {output_data['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Output File:\")\n",
    "print(f\"  - Filename: {OUTPUT_CSV}\")\n",
    "print(f\"  - Size: {Path(OUTPUT_CSV).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"  - Format: CSV (can be opened in Excel, pandas, etc.)\")\n",
    "\n",
    "print(f\"\\nâœ… Data collection complete! Ready for environment building.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
