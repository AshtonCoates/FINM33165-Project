{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collector for Earnings Event-Driven RL Trading\n",
    "\n",
    "This notebook processes raw price data and earnings dates into a structured dataset ready for environment building.\n",
    "\n",
    "## Overview\n",
    "1. Load raw price and earnings data\n",
    "2. Build event windows (N days before and after earnings)\n",
    "3. Calculate technical features\n",
    "4. Generate training/test splits\n",
    "5. Export to CSV format for environment building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Price Data: MAG7_hourly_5yr.csv\n",
      "  Earnings Data: mag7_earnings_dates_2019_2024.csv\n",
      "  Output CSV: earnings_events_data.csv\n",
      "  Event Window: 5 days before + 5 days after\n",
      "  Train/Test Split: 80%/20%\n"
     ]
    }
   ],
   "source": [
    "# ============ Data Paths ============\n",
    "RAW_PRICE_DATA = \"MAG7_hourly_5yr.csv\"  # Change this to your file path\n",
    "EARNINGS_DATA = \"mag7_earnings_dates_2019_2024.csv\"  # Change this to your file path\n",
    "OUTPUT_CSV = \"earnings_events_data.csv\"  # Output file\n",
    "\n",
    "# ============ Event Window Configuration ============\n",
    "DAYS_BEFORE = 5      # Days before earnings\n",
    "DAYS_AFTER = 5       # Days after earnings\n",
    "TOTAL_WINDOW = DAYS_BEFORE + DAYS_AFTER + 1  # Total days = 11\n",
    "\n",
    "# ============ Feature Calculation ============\n",
    "MOMENTUM_WINDOW = 3   # Calculate momentum using past 3 days\n",
    "VOLATILITY_WINDOW = 3  # Calculate volatility using past 3 days\n",
    "\n",
    "# ============ Trading Costs ============\n",
    "TRANSACTION_COST = 0.0005  # 0.05% (round-trip)\n",
    "\n",
    "# ============ Train/Test Split ============\n",
    "TRAIN_TEST_SPLIT = 0.80  # 80% training, 20% test\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Price Data: {RAW_PRICE_DATA}\")\n",
    "print(f\"  Earnings Data: {EARNINGS_DATA}\")\n",
    "print(f\"  Output CSV: {OUTPUT_CSV}\")\n",
    "print(f\"  Event Window: {DAYS_BEFORE} days before + {DAYS_AFTER} days after\")\n",
    "print(f\"  Train/Test Split: {TRAIN_TEST_SPLIT:.0%}/{1-TRAIN_TEST_SPLIT:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 01:43:06,376 - INFO - Loading raw data...\n",
      "2025-12-05 01:43:06,548 - INFO - ✓ Loaded 160,574 price records\n",
      "2025-12-05 01:43:06,548 - INFO - ✓ Loaded 144 earnings release dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price Data Sample:\n",
      "  symbol                 timestamp    open    high     low   close   volume  \\\n",
      "0   AAPL 2019-01-01 00:00:00+00:00  157.92  158.18  157.91  158.18  14683.0   \n",
      "1   AAPL 2019-01-02 09:00:00+00:00  154.40  154.70  153.01  154.70  24033.0   \n",
      "2   AAPL 2019-01-02 10:00:00+00:00  154.50  154.70  154.35  154.46   8605.0   \n",
      "3   AAPL 2019-01-02 11:00:00+00:00  154.58  154.65  154.10  154.46  18522.0   \n",
      "4   AAPL 2019-01-02 12:00:00+00:00  154.50  155.35  154.50  154.50  85291.0   \n",
      "\n",
      "   trade_count        vwap ticker  \n",
      "0         85.0  157.989075   AAPL  \n",
      "1        180.0  154.224457   AAPL  \n",
      "2         67.0  154.546683   AAPL  \n",
      "3        162.0  154.360502   AAPL  \n",
      "4        862.0  154.835064   AAPL  \n",
      "\n",
      "Price Data Info:\n",
      "  - Date Range: 2019-01-01 00:00:00+00:00 to 2024-12-31 00:00:00+00:00\n",
      "  - Tickers: ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
      "  - Total Records: 160,574\n",
      "\n",
      "Earnings Data Sample:\n",
      "  ticker earnings_date\n",
      "0   AAPL    2024-04-30\n",
      "1   AAPL    2024-08-01\n",
      "2   AAPL    2024-10-31\n",
      "3   AAPL    2025-02-06\n",
      "4   AAPL    2023-04-27\n",
      "\n",
      "Earnings Data Info:\n",
      "  - Date Range: 2018-10-24 00:00:00 to 2025-02-06 00:00:00\n",
      "  - Total Dates: 144\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading raw data...\")\n",
    "\n",
    "# Load price data\n",
    "price_data = pd.read_csv(RAW_PRICE_DATA)\n",
    "price_data['timestamp'] = pd.to_datetime(price_data['timestamp'])\n",
    "price_data = price_data.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Load earnings dates\n",
    "earnings_dates = pd.read_csv(EARNINGS_DATA)\n",
    "earnings_dates['earnings_date'] = pd.to_datetime(earnings_dates['earnings_date'])\n",
    "\n",
    "logger.info(f\"✓ Loaded {len(price_data):,} price records\")\n",
    "logger.info(f\"✓ Loaded {len(earnings_dates)} earnings release dates\")\n",
    "\n",
    "print(\"\\nPrice Data Sample:\")\n",
    "print(price_data.head())\n",
    "print(f\"\\nPrice Data Info:\")\n",
    "print(f\"  - Date Range: {price_data['timestamp'].min()} to {price_data['timestamp'].max()}\")\n",
    "print(f\"  - Tickers: {price_data['ticker'].unique().tolist()}\")\n",
    "print(f\"  - Total Records: {len(price_data):,}\")\n",
    "\n",
    "print(\"\\nEarnings Data Sample:\")\n",
    "print(earnings_dates.head())\n",
    "print(f\"\\nEarnings Data Info:\")\n",
    "print(f\"  - Date Range: {earnings_dates['earnings_date'].min()} to {earnings_dates['earnings_date'].max()}\")\n",
    "print(f\"  - Total Dates: {len(earnings_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Event Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 01:43:06,558 - INFO - Building event windows...\n",
      "2025-12-05 01:43:07,822 - INFO - ✓ Built 138 valid event windows\n",
      "2025-12-05 01:43:07,822 - INFO - ⊘ Skipped 6 events due to insufficient data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Event Windows Summary:\n",
      "  - Valid Events: 138\n",
      "  - Skipped Events: 6\n",
      "  - Total Data Points in Events: 14,815\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Building event windows...\")\n",
    "\n",
    "all_events_data = []\n",
    "valid_events_count = 0\n",
    "skipped_events_count = 0\n",
    "\n",
    "for _, earnings_row in earnings_dates.iterrows():\n",
    "    ticker = earnings_row['ticker']\n",
    "    earnings_date = pd.Timestamp(earnings_row['earnings_date'])\n",
    "    \n",
    "    # Get price data for this company\n",
    "    ticker_data = price_data[price_data['ticker'] == ticker].copy()\n",
    "    \n",
    "    # Define event window\n",
    "    window_start = earnings_date - timedelta(days=DAYS_BEFORE)\n",
    "    window_end = earnings_date + timedelta(days=DAYS_AFTER)\n",
    "    \n",
    "    # Extract window data - convert to date for comparison (avoid timezone issues)\n",
    "    ticker_data['timestamp_date'] = ticker_data['timestamp'].dt.date\n",
    "    earnings_start_date = window_start.date()\n",
    "    earnings_end_date = window_end.date()\n",
    "    \n",
    "    window_data = ticker_data[\n",
    "        (ticker_data['timestamp_date'] >= earnings_start_date) & \n",
    "        (ticker_data['timestamp_date'] <= earnings_end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # Check data completeness\n",
    "    if len(window_data) < 50:  # Approximately 5 days * 10 hours\n",
    "        skipped_events_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    window_data = window_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    window_data = window_data.drop('timestamp_date', axis=1)  # Remove helper column\n",
    "    \n",
    "    # Add event metadata\n",
    "    window_data['ticker_event'] = ticker\n",
    "    window_data['earnings_date'] = earnings_date\n",
    "    window_data['window_start'] = window_start\n",
    "    window_data['window_end'] = window_end\n",
    "    window_data['event_id'] = valid_events_count\n",
    "    \n",
    "    all_events_data.append(window_data)\n",
    "    valid_events_count += 1\n",
    "\n",
    "logger.info(f\"✓ Built {valid_events_count} valid event windows\")\n",
    "logger.info(f\"⊘ Skipped {skipped_events_count} events due to insufficient data\")\n",
    "\n",
    "print(f\"\\nEvent Windows Summary:\")\n",
    "print(f\"  - Valid Events: {valid_events_count}\")\n",
    "print(f\"  - Skipped Events: {skipped_events_count}\")\n",
    "print(f\"  - Total Data Points in Events: {sum(len(e) for e in all_events_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Technical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 01:43:07,830 - INFO - Calculating features for all events...\n",
      "2025-12-05 01:43:08,053 - INFO - ✓ Calculated features for 138 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Calculation Summary:\n",
      "  - Events with Features: 138\n",
      "  - Total Data Points: 14,815\n"
     ]
    }
   ],
   "source": [
    "def calculate_features(window_data, earnings_date):\n",
    "    \"\"\"\n",
    "    Calculate technical features for an event window.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - momentum: Pre-earnings momentum\n",
    "    - volatility: Pre-earnings volatility\n",
    "    - avg_volume: Average volume\n",
    "    - pre_close: Pre-earnings close price\n",
    "    - window_return: Window return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert earnings_date to date for comparison\n",
    "        if hasattr(earnings_date, 'date'):\n",
    "            earnings_date_compare = earnings_date.date()\n",
    "        else:\n",
    "            earnings_date_compare = earnings_date\n",
    "        \n",
    "        # Separate pre-earnings data\n",
    "        if 'timestamp_date' in window_data.columns:\n",
    "            pre_earnings = window_data[window_data['timestamp_date'] < earnings_date_compare]\n",
    "        else:\n",
    "            window_data_copy = window_data.copy()\n",
    "            window_data_copy['temp_date'] = window_data_copy['timestamp'].dt.date\n",
    "            pre_earnings = window_data_copy[window_data_copy['temp_date'] < earnings_date_compare]\n",
    "            pre_earnings = pre_earnings.drop('temp_date', axis=1)\n",
    "        \n",
    "        if len(pre_earnings) < 20:\n",
    "            return None\n",
    "        \n",
    "        # 1. Pre-earnings momentum (return over past N days)\n",
    "        pre_returns = pre_earnings['close'].pct_change().dropna()\n",
    "        momentum = pre_returns.tail(MOMENTUM_WINDOW).sum()\n",
    "        \n",
    "        # 2. Volatility (std dev over past N days)\n",
    "        volatility = pre_returns.tail(VOLATILITY_WINDOW).std()\n",
    "        \n",
    "        # 3. Average volume\n",
    "        avg_volume = pre_earnings['volume'].tail(VOLATILITY_WINDOW).mean()\n",
    "        \n",
    "        # 4. Pre-earnings closing price\n",
    "        pre_close = pre_earnings['close'].iloc[-1]\n",
    "        \n",
    "        # 5. Window return (full window)\n",
    "        pre_price = pre_earnings['close'].iloc[0]\n",
    "        \n",
    "        if 'timestamp_date' in window_data.columns:\n",
    "            post_earnings = window_data[window_data['timestamp_date'] > earnings_date_compare]\n",
    "        else:\n",
    "            window_data_copy = window_data.copy()\n",
    "            window_data_copy['temp_date'] = window_data_copy['timestamp'].dt.date\n",
    "            post_earnings = window_data_copy[window_data_copy['temp_date'] > earnings_date_compare]\n",
    "            post_earnings = post_earnings.drop('temp_date', axis=1)\n",
    "        \n",
    "        if len(post_earnings) > 0:\n",
    "            post_price = post_earnings['close'].iloc[-1]\n",
    "            window_return = (post_price - pre_price) / pre_price\n",
    "        else:\n",
    "            window_return = 0\n",
    "        \n",
    "        return {\n",
    "            'momentum': momentum,\n",
    "            'volatility': volatility if volatility > 0 else 0.01,\n",
    "            'avg_volume': avg_volume,\n",
    "            'pre_close': pre_close,\n",
    "            'window_return': window_return\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Feature calculation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "logger.info(\"Calculating features for all events...\")\n",
    "\n",
    "# Add features to each event\n",
    "processed_events = []\n",
    "for event_data in all_events_data:\n",
    "    ticker = event_data['ticker_event'].iloc[0]\n",
    "    earnings_date = event_data['earnings_date'].iloc[0]\n",
    "    \n",
    "    features = calculate_features(event_data, earnings_date)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Add features to each row in the event\n",
    "        event_data['momentum'] = features['momentum']\n",
    "        event_data['volatility'] = features['volatility']\n",
    "        event_data['avg_volume_event'] = features['avg_volume']\n",
    "        event_data['pre_close'] = features['pre_close']\n",
    "        event_data['window_return'] = features['window_return']\n",
    "        \n",
    "        processed_events.append(event_data)\n",
    "\n",
    "logger.info(f\"✓ Calculated features for {len(processed_events)} events\")\n",
    "\n",
    "print(f\"\\nFeature Calculation Summary:\")\n",
    "print(f\"  - Events with Features: {len(processed_events)}\")\n",
    "print(f\"  - Total Data Points: {sum(len(e) for e in processed_events):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 01:43:08,089 - INFO - ✓ Train/Test Split:\n",
      "2025-12-05 01:43:08,089 - INFO -   - Training events: 110\n",
      "2025-12-05 01:43:08,090 - INFO -   - Test events: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test Split Summary:\n",
      "  - Training Events: 110\n",
      "  - Test Events: 28\n",
      "  - Training Rows: 11,788\n",
      "  - Test Rows: 3,027\n",
      "\n",
      "Data Sample:\n",
      "  symbol                 timestamp      open      high       low     close  \\\n",
      "0   AAPL 2024-04-25 08:00:00+00:00  168.8300  169.1400  168.6200  168.8500   \n",
      "1   AAPL 2024-04-25 09:00:00+00:00  168.8200  169.1500  168.8200  169.1300   \n",
      "2   AAPL 2024-04-25 10:00:00+00:00  169.0000  169.0900  168.8100  168.8100   \n",
      "3   AAPL 2024-04-25 11:00:00+00:00  168.7400  169.3100  168.6300  169.1500   \n",
      "4   AAPL 2024-04-25 12:00:00+00:00  168.7966  169.9600  168.1400  168.3400   \n",
      "5   AAPL 2024-04-25 13:00:00+00:00  168.3500  170.6100  167.8900  169.0100   \n",
      "6   AAPL 2024-04-25 14:00:00+00:00  169.0200  169.4900  168.4700  168.5750   \n",
      "7   AAPL 2024-04-25 15:00:00+00:00  168.5700  168.8700  168.1511  168.5500   \n",
      "8   AAPL 2024-04-25 16:00:00+00:00  168.5400  169.5383  168.5000  169.3599   \n",
      "9   AAPL 2024-04-25 17:00:00+00:00  169.3600  169.6100  168.8600  169.5501   \n",
      "\n",
      "      volume  trade_count        vwap ticker  ... earnings_date window_start  \\\n",
      "0    38844.0        890.0  168.812168   AAPL  ...    2024-04-30   2024-04-25   \n",
      "1     6809.0        185.0  168.974191   AAPL  ...    2024-04-30   2024-04-25   \n",
      "2     6282.0        193.0  168.937274   AAPL  ...    2024-04-30   2024-04-25   \n",
      "3    36038.0        663.0  169.007082   AAPL  ...    2024-04-30   2024-04-25   \n",
      "4   281856.0       6473.0  168.906938   AAPL  ...    2024-04-30   2024-04-25   \n",
      "5  8646760.0      99441.0  169.784752   AAPL  ...    2024-04-30   2024-04-25   \n",
      "6  6688395.0     145660.0  168.898580   AAPL  ...    2024-04-30   2024-04-25   \n",
      "7  4532644.0     102995.0  168.510692   AAPL  ...    2024-04-30   2024-04-25   \n",
      "8  4000417.0      45312.0  169.014879   AAPL  ...    2024-04-30   2024-04-25   \n",
      "9  3659769.0      41871.0  169.245271   AAPL  ...    2024-04-30   2024-04-25   \n",
      "\n",
      "  window_end event_id  momentum  volatility  avg_volume_event  pre_close  \\\n",
      "0 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "1 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "2 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "3 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "4 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "5 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "6 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "7 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "8 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "9 2024-05-05        0  0.000116     0.00052     609035.666667     173.52   \n",
      "\n",
      "   window_return  split  \n",
      "0       0.088422  train  \n",
      "1       0.088422  train  \n",
      "2       0.088422  train  \n",
      "3       0.088422  train  \n",
      "4       0.088422  train  \n",
      "5       0.088422  train  \n",
      "6       0.088422  train  \n",
      "7       0.088422  train  \n",
      "8       0.088422  train  \n",
      "9       0.088422  train  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine all events into single dataframe\n",
    "combined_data = pd.concat(processed_events, ignore_index=True)\n",
    "\n",
    "# Get unique events for train/test split\n",
    "unique_events = combined_data[['event_id', 'earnings_date']].drop_duplicates('event_id')\n",
    "\n",
    "# Split events (not individual rows)\n",
    "train_size = int(len(unique_events) * TRAIN_TEST_SPLIT)\n",
    "train_event_ids = unique_events.iloc[:train_size]['event_id'].values\n",
    "test_event_ids = unique_events.iloc[train_size:]['event_id'].values\n",
    "\n",
    "# Add split label\n",
    "combined_data['split'] = combined_data['event_id'].apply(\n",
    "    lambda x: 'train' if x in train_event_ids else 'test'\n",
    ")\n",
    "\n",
    "logger.info(f\"✓ Train/Test Split:\")\n",
    "logger.info(f\"  - Training events: {len(train_event_ids)}\")\n",
    "logger.info(f\"  - Test events: {len(test_event_ids)}\")\n",
    "\n",
    "print(f\"\\nTrain/Test Split Summary:\")\n",
    "print(f\"  - Training Events: {len(train_event_ids)}\")\n",
    "print(f\"  - Test Events: {len(test_event_ids)}\")\n",
    "print(f\"  - Training Rows: {len(combined_data[combined_data['split'] == 'train']):,}\")\n",
    "print(f\"  - Test Rows: {len(combined_data[combined_data['split'] == 'test']):,}\")\n",
    "\n",
    "print(f\"\\nData Sample:\")\n",
    "print(combined_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Statistics ===\n",
      "\n",
      "Training Set Features:\n",
      "         momentum  volatility  window_return\n",
      "count  110.000000  110.000000     110.000000\n",
      "mean     0.001282    0.002467       0.016606\n",
      "std      0.007561    0.003193       0.086952\n",
      "min     -0.038086    0.000095      -0.185032\n",
      "25%     -0.000925    0.000817      -0.037597\n",
      "50%      0.001308    0.001486       0.017194\n",
      "75%      0.003705    0.002634       0.054115\n",
      "max      0.036428    0.019010       0.342879\n",
      "\n",
      "Test Set Features:\n",
      "        momentum  volatility  window_return\n",
      "count  28.000000   28.000000      28.000000\n",
      "mean    0.005437    0.004203       0.008454\n",
      "std     0.009150    0.003863       0.114102\n",
      "min    -0.013961    0.000343      -0.263640\n",
      "25%    -0.000304    0.001458      -0.042332\n",
      "50%     0.003732    0.002493      -0.001243\n",
      "75%     0.009840    0.006458       0.078706\n",
      "max     0.029653    0.013927       0.268753\n",
      "\n",
      "Window Return Distribution:\n",
      "  - Positive Returns: 76 events\n",
      "  - Negative Returns: 62 events\n",
      "  - Mean Return: 0.0150\n",
      "  - Median Return: 0.0160\n"
     ]
    }
   ],
   "source": [
    "# Get unique features per event\n",
    "event_features = combined_data.groupby('event_id').agg({\n",
    "    'momentum': 'first',\n",
    "    'volatility': 'first',\n",
    "    'avg_volume_event': 'first',\n",
    "    'window_return': 'first',\n",
    "    'split': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\n=== Feature Statistics ===\")\n",
    "print(\"\\nTraining Set Features:\")\n",
    "train_features = event_features[event_features['split'] == 'train']\n",
    "print(train_features[['momentum', 'volatility', 'window_return']].describe())\n",
    "\n",
    "print(\"\\nTest Set Features:\")\n",
    "test_features = event_features[event_features['split'] == 'test']\n",
    "print(test_features[['momentum', 'volatility', 'window_return']].describe())\n",
    "\n",
    "print(f\"\\nWindow Return Distribution:\")\n",
    "print(f\"  - Positive Returns: {(event_features['window_return'] > 0).sum()} events\")\n",
    "print(f\"  - Negative Returns: {(event_features['window_return'] < 0).sum()} events\")\n",
    "print(f\"  - Mean Return: {event_features['window_return'].mean():.4f}\")\n",
    "print(f\"  - Median Return: {event_features['window_return'].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare CSV for Environment Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 01:43:08,268 - INFO - ✓ Saved CSV to earnings_events_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CSV Export Complete!\n",
      "\n",
      "File: earnings_events_data.csv\n",
      "\n",
      "Columns:\n",
      "   1. ticker_event\n",
      "   2. earnings_date\n",
      "   3. window_start\n",
      "   4. window_end\n",
      "   5. event_id\n",
      "   6. timestamp\n",
      "   7. open\n",
      "   8. high\n",
      "   9. low\n",
      "  10. close\n",
      "  11. volume\n",
      "  12. momentum\n",
      "  13. volatility\n",
      "  14. avg_volume_event\n",
      "  15. pre_close\n",
      "  16. window_return\n",
      "  17. split\n",
      "\n",
      "Dataset Size:\n",
      "  - Total Rows: 14,815\n",
      "  - Total Columns: 17\n",
      "  - File Size: 2847.57 KB\n",
      "\n",
      "First Few Rows:\n",
      "  ticker_event earnings_date window_start window_end  event_id  \\\n",
      "0         AAPL    2024-04-30   2024-04-25 2024-05-05         0   \n",
      "1         AAPL    2024-04-30   2024-04-25 2024-05-05         0   \n",
      "2         AAPL    2024-04-30   2024-04-25 2024-05-05         0   \n",
      "3         AAPL    2024-04-30   2024-04-25 2024-05-05         0   \n",
      "4         AAPL    2024-04-30   2024-04-25 2024-05-05         0   \n",
      "\n",
      "                  timestamp      open    high     low   close    volume  \\\n",
      "0 2024-04-25 08:00:00+00:00  168.8300  169.14  168.62  168.85   38844.0   \n",
      "1 2024-04-25 09:00:00+00:00  168.8200  169.15  168.82  169.13    6809.0   \n",
      "2 2024-04-25 10:00:00+00:00  169.0000  169.09  168.81  168.81    6282.0   \n",
      "3 2024-04-25 11:00:00+00:00  168.7400  169.31  168.63  169.15   36038.0   \n",
      "4 2024-04-25 12:00:00+00:00  168.7966  169.96  168.14  168.34  281856.0   \n",
      "\n",
      "   momentum  volatility  avg_volume_event  pre_close  window_return  split  \n",
      "0  0.000116     0.00052     609035.666667     173.52       0.088422  train  \n",
      "1  0.000116     0.00052     609035.666667     173.52       0.088422  train  \n",
      "2  0.000116     0.00052     609035.666667     173.52       0.088422  train  \n",
      "3  0.000116     0.00052     609035.666667     173.52       0.088422  train  \n",
      "4  0.000116     0.00052     609035.666667     173.52       0.088422  train  \n",
      "\n",
      "Last Few Rows:\n",
      "      ticker_event earnings_date window_start window_end  event_id  \\\n",
      "14810         META    2020-01-29   2020-01-24 2020-02-03       137   \n",
      "14811         META    2020-01-29   2020-01-24 2020-02-03       137   \n",
      "14812         META    2020-01-29   2020-01-24 2020-02-03       137   \n",
      "14813         META    2020-01-29   2020-01-24 2020-02-03       137   \n",
      "14814         META    2020-01-29   2020-01-24 2020-02-03       137   \n",
      "\n",
      "                      timestamp     open      high     low    close  \\\n",
      "14810 2020-02-03 19:00:00+00:00  204.020  204.3600  203.64  203.725   \n",
      "14811 2020-02-03 20:00:00+00:00  203.725  204.3000  203.50  204.190   \n",
      "14812 2020-02-03 21:00:00+00:00  204.190  204.3000  203.00  203.190   \n",
      "14813 2020-02-03 22:00:00+00:00  203.370  203.4399  203.14  203.300   \n",
      "14814 2020-02-03 23:00:00+00:00  203.210  203.4400  202.80  203.440   \n",
      "\n",
      "          volume  momentum  volatility  avg_volume_event  pre_close  \\\n",
      "14810  1106394.0  0.005368    0.002409         1069680.0     218.96   \n",
      "14811  1974139.0  0.005368    0.002409         1069680.0     218.96   \n",
      "14812  2325067.0  0.005368    0.002409         1069680.0     218.96   \n",
      "14813    13256.0  0.005368    0.002409         1069680.0     218.96   \n",
      "14814     9439.0  0.005368    0.002409         1069680.0     218.96   \n",
      "\n",
      "       window_return split  \n",
      "14810      -0.076113  test  \n",
      "14811      -0.076113  test  \n",
      "14812      -0.076113  test  \n",
      "14813      -0.076113  test  \n",
      "14814      -0.076113  test  \n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns for environment\n",
    "csv_columns = [\n",
    "    'ticker_event',\n",
    "    'earnings_date',\n",
    "    'window_start',\n",
    "    'window_end',\n",
    "    'event_id',\n",
    "    'timestamp',\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'volume',\n",
    "    'momentum',\n",
    "    'volatility',\n",
    "    'avg_volume_event',\n",
    "    'pre_close',\n",
    "    'window_return',\n",
    "    'split'\n",
    "]\n",
    "\n",
    "output_data = combined_data[csv_columns].copy()\n",
    "\n",
    "# Sort by event and timestamp\n",
    "output_data = output_data.sort_values(['event_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_data.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "logger.info(f\"✓ Saved CSV to {OUTPUT_CSV}\")\n",
    "\n",
    "print(f\"\\n CSV Export Complete!\")\n",
    "print(f\"\\nFile: {OUTPUT_CSV}\")\n",
    "print(f\"\\nColumns:\")\n",
    "for i, col in enumerate(csv_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nDataset Size:\")\n",
    "print(f\"  - Total Rows: {len(output_data):,}\")\n",
    "print(f\"  - Total Columns: {len(csv_columns)}\")\n",
    "print(f\"  - File Size: {Path(OUTPUT_CSV).stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nFirst Few Rows:\")\n",
    "print(output_data.head())\n",
    "\n",
    "print(f\"\\nLast Few Rows:\")\n",
    "print(output_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Quality Check ===\n",
      "\n",
      "Missing Values:\n",
      "  ✓ No missing values\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Split Distribution:\n",
      "split\n",
      "train    11788\n",
      "test      3027\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data Types:\n",
      "ticker_event                     object\n",
      "earnings_date            datetime64[ns]\n",
      "window_start             datetime64[ns]\n",
      "window_end               datetime64[ns]\n",
      "event_id                          int64\n",
      "timestamp           datetime64[ns, UTC]\n",
      "open                            float64\n",
      "high                            float64\n",
      "low                             float64\n",
      "close                           float64\n",
      "volume                          float64\n",
      "momentum                        float64\n",
      "volatility                      float64\n",
      "avg_volume_event                float64\n",
      "pre_close                       float64\n",
      "window_return                   float64\n",
      "split                            object\n",
      "dtype: object\n",
      "\n",
      "Price Range Check:\n",
      "  - Close Price Min: $92.04\n",
      "  - Close Price Max: $3702.00\n",
      "  - Volume Min: 100.0\n",
      "  - Volume Max: 58,777,355.0\n",
      "\n",
      " Data Quality Check Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Data Quality Check ===\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = output_data.isnull().sum()\n",
    "print(f\"\\nMissing Values:\")\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    print(f\"  - {col}: {missing_values[col]}\")\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"  ✓ No missing values\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = output_data.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "# Check split distribution\n",
    "print(f\"\\nSplit Distribution:\")\n",
    "print(output_data['split'].value_counts())\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData Types:\")\n",
    "print(output_data.dtypes)\n",
    "\n",
    "# Check for outliers\n",
    "print(f\"\\nPrice Range Check:\")\n",
    "print(f\"  - Close Price Min: ${output_data['close'].min():.2f}\")\n",
    "print(f\"  - Close Price Max: ${output_data['close'].max():.2f}\")\n",
    "print(f\"  - Volume Min: {output_data['volume'].min():,}\")\n",
    "print(f\"  - Volume Max: {output_data['volume'].max():,}\")\n",
    "\n",
    "print(f\"\\n Data Quality Check Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      " Dataset Statistics:\n",
      "  - Total Events: 138\n",
      "  - Training Events: 110\n",
      "  - Test Events: 28\n",
      "  - Total Data Points: 14,815\n",
      "  - Unique Tickers: 6\n",
      "  - Date Range: 2019-01-25 00:00:00+00:00 to 2024-11-05 23:00:00+00:00\n",
      "\n",
      " Output File:\n",
      "  - Filename: earnings_events_data.csv\n",
      "  - Size: 2.78 MB\n",
      "  - Format: CSV (can be opened in Excel, pandas, etc.)\n",
      "\n",
      " Data collection complete! Ready for environment building.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Dataset Statistics:\")\n",
    "print(f\"  - Total Events: {output_data['event_id'].nunique()}\")\n",
    "print(f\"  - Training Events: {len(output_data[output_data['split'] == 'train']['event_id'].unique())}\")\n",
    "print(f\"  - Test Events: {len(output_data[output_data['split'] == 'test']['event_id'].unique())}\")\n",
    "print(f\"  - Total Data Points: {len(output_data):,}\")\n",
    "print(f\"  - Unique Tickers: {output_data['ticker_event'].nunique()}\")\n",
    "print(f\"  - Date Range: {output_data['timestamp'].min()} to {output_data['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\n Output File:\")\n",
    "print(f\"  - Filename: {OUTPUT_CSV}\")\n",
    "print(f\"  - Size: {Path(OUTPUT_CSV).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"  - Format: CSV (can be opened in Excel, pandas, etc.)\")\n",
    "\n",
    "print(f\"\\n Data collection complete! Ready for environment building.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
