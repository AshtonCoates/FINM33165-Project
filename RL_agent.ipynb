{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de354ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#########################################\n",
    "# ------------- Environment -------------\n",
    "#########################################\n",
    "class EarningsEventEnv(gym.Env):\n",
    "    '''Trading environment for earnings event-driven strategy.'''\n",
    "    \n",
    "    def __init__(self, event_data, transaction_cost=0.0005, initial_cash=10000):\n",
    "        super(EarningsEventEnv, self).__init__()\n",
    "        \n",
    "        self.event_data = event_data.sort_values('timestamp').reset_index(drop=True).copy()\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.initial_cash = initial_cash\n",
    "        \n",
    "        # Safety: ensure columns exist\n",
    "        required = ['ticker_event','earnings_date','event_id','momentum','volatility','pre_close','close','timestamp']\n",
    "        for c in required:\n",
    "            if c not in self.event_data.columns:\n",
    "                raise ValueError(f\"Missing column in event_data: {c}\")\n",
    "        \n",
    "        # Extract metadata (first row)\n",
    "        self.ticker = event_data['ticker_event'].iloc[0]\n",
    "        self.earnings_date = event_data['earnings_date'].iloc[0]\n",
    "        self.event_id = event_data['event_id'].iloc[0]\n",
    "        self.momentum = float(event_data['momentum'].iloc[0])\n",
    "        self.volatility = float(event_data['volatility'].iloc[0])\n",
    "        self.pre_close = float(event_data['pre_close'].iloc[0])\n",
    "        \n",
    "        # Define spaces\n",
    "        self.action_space = spaces.Discrete(3)  # 0=HOLD, 1=BUY (enter long), 2=SELL (exit long)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 or 1 (long)\n",
    "        self.cash = initial_cash\n",
    "        self.trades = []\n",
    "    \n",
    "    def reset(self, seed=None, return_info=False):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.trades = []\n",
    "        obs = self._get_observation()\n",
    "        if return_info:\n",
    "            return obs, {}\n",
    "        return obs\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        if self.current_step >= len(self.event_data):\n",
    "            current_price = float(self.event_data['close'].iloc[-1])\n",
    "        else:\n",
    "            current_price = float(self.event_data['close'].iloc[self.current_step])\n",
    "        \n",
    "        portfolio_value = self.cash + (self.position * current_price)\n",
    "        window_pnl = (portfolio_value - self.initial_cash) / self.initial_cash\n",
    "        \n",
    "        state = np.array([\n",
    "            self.momentum,\n",
    "            self.volatility,\n",
    "            self.pre_close,\n",
    "            current_price,\n",
    "            float(self.position),\n",
    "            float(self.cash),\n",
    "            float(window_pnl)\n",
    "        ], dtype=np.float32)\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action: 0=hold,1=buy,2=sell\n",
    "        if self.current_step >= len(self.event_data):\n",
    "            # Already done; return terminal observation\n",
    "            observation = self._get_observation()\n",
    "            return observation, 0.0, True, {}\n",
    "        \n",
    "        current_price = float(self.event_data['close'].iloc[self.current_step])\n",
    "        reward = 0.0\n",
    "        \n",
    "        if action == 1 and self.position == 0:\n",
    "            cost = current_price * (1.0 + self.transaction_cost)\n",
    "            if self.cash >= cost:\n",
    "                self.position = 1\n",
    "                self.cash -= cost\n",
    "                self.trades.append({'action': 'buy', 'price': current_price, 'step': self.current_step})\n",
    "        elif action == 2 and self.position == 1:\n",
    "            proceeds = current_price * (1.0 - self.transaction_cost)\n",
    "            self.cash += proceeds\n",
    "            self.position = 0\n",
    "            self.trades.append({'action': 'sell', 'price': current_price, 'step': self.current_step})\n",
    "        # else: hold or illegal action (ignored)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.event_data))\n",
    "        \n",
    "        # If done and still long -> auto close at final price with costs\n",
    "        if done and self.position == 1:\n",
    "            final_price = float(self.event_data['close'].iloc[-1])\n",
    "            self.cash += final_price * (1.0 - self.transaction_cost)\n",
    "            self.position = 0\n",
    "            self.trades.append({'action': 'auto_sell', 'price': final_price, 'step': self.current_step-1})\n",
    "        \n",
    "        if done:\n",
    "            final_value = self.cash\n",
    "            reward = (final_value - self.initial_cash) / self.initial_cash\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, float(reward), bool(done), {}\n",
    "    \n",
    "    def get_window_pnl(self):\n",
    "        portfolio = self.cash + (self.position * float(self.event_data['close'].iloc[-1]))\n",
    "        return (portfolio - self.initial_cash) / self.initial_cash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# -------------------- Policy (Actor-Critic) -----------------------\n",
    "####################################################################\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_dim=7, n_actions=3, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden, n_actions)  # logits\n",
    "        self.critic = nn.Linear(hidden, 1)         # state value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        logits = self.actor(z)\n",
    "        value = self.critic(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "####################################################################\n",
    "# ---------------------- Agent utilities ---------------------------\n",
    "####################################################################\n",
    "def select_action(model, state):\n",
    "    \"\"\"Return action (int), log_prob (tensor), value (tensor), entropy (tensor)\"\"\"\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)  # [1,obs]\n",
    "    logits, value = model(state_t)   # logits: [1, n_actions], value: [1]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    dist = torch.distributions.Categorical(probs=probs)\n",
    "    action = dist.sample()\n",
    "    logp = dist.log_prob(action)\n",
    "    entropy = dist.entropy()\n",
    "    return int(action.item()), logp.squeeze(0), value.squeeze(0), entropy.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6852aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# -------------------- Training Loop -------------------------------\n",
    "####################################################################\n",
    "def train(train_envs, test_envs=None, epochs=50, events_per_epoch=64, lr=3e-4, gamma=1.0,\n",
    "          entropy_coef=0.01, value_coef=0.5, max_grad_norm=0.5, save_path='pg_agent.pth'):\n",
    "    \"\"\"\n",
    "    train_envs: dict[event_id] -> EarningsEventEnv\n",
    "    test_envs: dict (optional)\n",
    "    \"\"\"\n",
    "    obs_dim = 7\n",
    "    n_actions = 3\n",
    "    model = ActorCriticNet(obs_dim=obs_dim, n_actions=n_actions).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    event_ids = list(train_envs.keys())\n",
    "    if len(event_ids) == 0:\n",
    "        raise ValueError(\"No training events provided\")\n",
    "    \n",
    "    best_eval = -float('inf')\n",
    "    history = {'train_return': [], 'eval_return': []}\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_returns = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # sample events for this epoch (with replacement if not enough)\n",
    "        chosen = [random.choice(event_ids) for _ in range(events_per_epoch)]\n",
    "        \n",
    "        for eid in chosen:\n",
    "            env = train_envs[eid]\n",
    "            state = env.reset()\n",
    "            \n",
    "            log_probs = []\n",
    "            values = []\n",
    "            entropies = []\n",
    "            rewards = []\n",
    "            done = False\n",
    "            \n",
    "            # roll out entire event (episode)\n",
    "            while not done:\n",
    "                action, logp, value, entropy = select_action(model, state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                log_probs.append(logp)\n",
    "                values.append(value)\n",
    "                entropies.append(entropy)\n",
    "                rewards.append(reward)  # mostly 0 until final step where reward=window pnl\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            # for REINFORCE with baseline: compute return for each step (rewards are zeros except final)\n",
    "            # but handle generic discounted case in case you modify reward shaping\n",
    "            returns = []\n",
    "            R = 0.0\n",
    "            # Since rewards are given at end, iterating backwards works fine.\n",
    "            for r in reversed(rewards):\n",
    "                R = r + gamma * R\n",
    "                returns.insert(0, R)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "            values = torch.stack(values)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            entropies = torch.stack(entropies)\n",
    "            \n",
    "            advantages = returns - values.detach()\n",
    "            \n",
    "            # actor loss (policy gradient with advantage)\n",
    "            actor_loss = -(log_probs * advantages).mean()\n",
    "            # critic loss (value MSE)\n",
    "            critic_loss = F.mse_loss(values, returns)\n",
    "            # entropy bonus\n",
    "            entropy_loss = -entropies.mean()\n",
    "            \n",
    "            loss = actor_loss + value_coef * critic_loss + entropy_coef * entropy_loss\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            opt.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_returns.append(returns[-1].item())  # final event return (window pnl)\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_return = np.mean(epoch_returns)\n",
    "        history['train_return'].append(avg_return)\n",
    "        \n",
    "        # optional evaluation on test set\n",
    "        eval_return = None\n",
    "        if test_envs:\n",
    "            model.eval()\n",
    "            eval_returns = []\n",
    "            with torch.no_grad():\n",
    "                # evaluate on up to 128 test events (or all if fewer)\n",
    "                test_ids = list(test_envs.keys())\n",
    "                if len(test_ids) > 128:\n",
    "                    eval_sample = random.sample(test_ids, 128)\n",
    "                else:\n",
    "                    eval_sample = test_ids\n",
    "                for eid in eval_sample:\n",
    "                    env = test_envs[eid]\n",
    "                    s = env.reset()\n",
    "                    done = False\n",
    "                    while not done:\n",
    "                        # greedy/policy sample: we sample to keep stochasticity\n",
    "                        a, _, _, _ = select_action(model, s)\n",
    "                        s, r, done, _ = env.step(a)\n",
    "                    eval_returns.append(env.get_window_pnl())\n",
    "            eval_return = float(np.mean(eval_returns)) if len(eval_returns) > 0 else 0.0\n",
    "            history['eval_return'].append(eval_return)\n",
    "            # save best\n",
    "            if eval_return > best_eval:\n",
    "                best_eval = eval_return\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            # save periodically\n",
    "            if epoch % 10 == 0:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch:3d} | loss {avg_loss:.4f} | train_return {avg_return:.4f} | eval_return {eval_return if eval_return is not None else 'N/A'} | time {elapsed:.1f}s\")\n",
    "    \n",
    "    # final save\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"Training complete. Model saved to\", save_path)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58792c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# ----------------------- Utility functions ------------------------\n",
    "####################################################################\n",
    "def build_envs_from_csv(csv_path, transaction_cost=0.0005, initial_cash=10000, split_col='split'):\n",
    "    \"\"\"\n",
    "    Expects CSV with columns per Appendix 1 and a 'split' column with values 'train'/'test'.\n",
    "    Returns train_envs, test_envs as dicts of EarningsEventEnv instances keyed by event_id.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['earnings_date'] = pd.to_datetime(df['earnings_date'])\n",
    "    \n",
    "    # must have event_id column\n",
    "    if 'event_id' not in df.columns:\n",
    "        raise ValueError(\"CSV must include 'event_id' column to group by events.\")\n",
    "    if split_col not in df.columns:\n",
    "        # If no split provided, create a random split\n",
    "        unique_events = df['event_id'].unique()\n",
    "        rng = np.random.RandomState(0)\n",
    "        mask = {eid: ('train' if rng.rand() < 0.8 else 'test') for eid in unique_events}\n",
    "        df['split'] = df['event_id'].map(mask)\n",
    "    events = df.groupby('event_id').first().reset_index()\n",
    "    train_events = events[events['split'] == 'train']\n",
    "    test_events = events[events['split'] == 'test']\n",
    "    \n",
    "    train_envs = {}\n",
    "    for event_id in train_events['event_id']:\n",
    "        event_data = df[df['event_id'] == event_id].copy().reset_index(drop=True)\n",
    "        train_envs[event_id] = EarningsEventEnv(event_data, transaction_cost=transaction_cost, initial_cash=initial_cash)\n",
    "    \n",
    "    test_envs = {}\n",
    "    for event_id in test_events['event_id']:\n",
    "        event_data = df[df['event_id'] == event_id].copy().reset_index(drop=True)\n",
    "        test_envs[event_id] = EarningsEventEnv(event_data, transaction_cost=transaction_cost, initial_cash=initial_cash)\n",
    "    \n",
    "    return df, train_envs, test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV: earnings_events_data.csv\n",
      "Loaded 14,815 rows. Train events: 110, Test events: 28\n",
      "Epoch   1 | loss 2549.6211 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch   2 | loss 445.5890 | train_return 0.0006 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch   3 | loss 224.0462 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch   4 | loss 449.8268 | train_return -0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch   5 | loss 200.6727 | train_return 0.0015 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch   6 | loss 170.8739 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch   7 | loss 140.4495 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch   8 | loss 328.2689 | train_return 0.0014 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch   9 | loss 278.8358 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  10 | loss 179.9735 | train_return 0.0028 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  11 | loss 365.6123 | train_return 0.0025 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  12 | loss 425.4744 | train_return 0.0015 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  13 | loss 148.0650 | train_return 0.0011 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  14 | loss 164.4397 | train_return 0.0012 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  15 | loss 131.2693 | train_return 0.0019 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  16 | loss 314.1720 | train_return 0.0002 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  17 | loss 87.5906 | train_return 0.0011 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  18 | loss 98.4662 | train_return 0.0004 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  19 | loss 103.7972 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  20 | loss 70.5943 | train_return 0.0024 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  21 | loss 105.6629 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  22 | loss 111.5955 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  23 | loss 71.4694 | train_return -0.0000 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  24 | loss 118.9676 | train_return 0.0010 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  25 | loss 85.3929 | train_return -0.0001 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  26 | loss 155.1222 | train_return 0.0023 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  27 | loss 171.3566 | train_return 0.0021 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  28 | loss 111.0748 | train_return 0.0020 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  29 | loss 106.4830 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  30 | loss 140.3715 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  31 | loss 127.7621 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  32 | loss 44.4069 | train_return -0.0004 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  33 | loss 34.9245 | train_return 0.0015 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  34 | loss 27.6816 | train_return 0.0013 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  35 | loss 217.5724 | train_return 0.0004 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  36 | loss 41.6949 | train_return 0.0006 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  37 | loss 85.4898 | train_return 0.0010 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  38 | loss 36.7507 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  39 | loss 37.3063 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  40 | loss 47.0521 | train_return 0.0012 | eval_return -1.9240948214311335e-06 | time 2.9s\n",
      "Epoch  41 | loss 36.4504 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  42 | loss 38.5105 | train_return 0.0012 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  43 | loss 23.4024 | train_return 0.0021 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  44 | loss 44.2244 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  45 | loss 22.8515 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  46 | loss 53.2528 | train_return 0.0014 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  47 | loss 32.8531 | train_return 0.0012 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  48 | loss 35.6655 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  49 | loss 51.0668 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  50 | loss 52.4749 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  51 | loss 19.7605 | train_return 0.0017 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  52 | loss 30.0944 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  53 | loss 81.0255 | train_return 0.0014 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  54 | loss 18.9718 | train_return 0.0006 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  55 | loss 28.3048 | train_return 0.0004 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  56 | loss 31.9107 | train_return 0.0008 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  57 | loss 28.7656 | train_return 0.0010 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  58 | loss 11.4908 | train_return 0.0009 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  59 | loss 22.3614 | train_return 0.0025 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  60 | loss 17.5325 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  61 | loss 30.3110 | train_return -0.0007 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  62 | loss 33.2989 | train_return 0.0005 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  63 | loss 24.4436 | train_return 0.0027 | eval_return -1.9240948214311335e-06 | time 2.9s\n",
      "Epoch  64 | loss 18.7559 | train_return 0.0015 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  65 | loss 20.0979 | train_return 0.0021 | eval_return -1.9240948214311335e-06 | time 2.9s\n",
      "Epoch  66 | loss 21.3614 | train_return -0.0006 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  67 | loss 10.1497 | train_return 0.0003 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  68 | loss 11.7370 | train_return 0.0015 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  69 | loss 30.5978 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  70 | loss 25.3438 | train_return 0.0013 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  71 | loss 16.7659 | train_return 0.0016 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  72 | loss 13.0157 | train_return -0.0003 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  73 | loss 13.6123 | train_return 0.0001 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  74 | loss 12.9595 | train_return 0.0014 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  75 | loss 10.9149 | train_return 0.0007 | eval_return -1.9240948214311335e-06 | time 3.1s\n",
      "Epoch  76 | loss 19.6626 | train_return 0.0011 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  77 | loss 10.1022 | train_return 0.0010 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  78 | loss 23.0762 | train_return 0.0001 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  79 | loss 15.1264 | train_return 0.0004 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Epoch  80 | loss 7.0286 | train_return 0.0003 | eval_return -1.9240948214311335e-06 | time 3.0s\n",
      "Training complete. Model saved to earnings_pg_agent.pth\n",
      "\n",
      "Sample evaluation on 10 test events (stochastic policy):\n",
      "Event 110 | PnL 0.0008 | trades [{'action': 'buy', 'price': 170.5, 'step': 0}, {'action': 'auto_sell', 'price': 179.0, 'step': 107}]\n",
      "Event 111 | PnL -0.0001 | trades [{'action': 'buy', 'price': 204.78, 'step': 0}, {'action': 'auto_sell', 'price': 204.3, 'step': 107}]\n",
      "Event 112 | PnL -0.0000 | trades [{'action': 'buy', 'price': 105.75, 'step': 0}, {'action': 'auto_sell', 'price': 105.51, 'step': 107}]\n",
      "Event 113 | PnL 0.0006 | trades [{'action': 'buy', 'price': 123.14, 'step': 0}, {'action': 'auto_sell', 'price': 129.75, 'step': 91}]\n",
      "Event 114 | PnL -0.0000 | trades [{'action': 'buy', 'price': 139.2, 'step': 0}, {'action': 'auto_sell', 'price': 139.17, 'step': 106}]\n",
      "Event 115 | PnL -0.0066 | trades [{'action': 'buy', 'price': 497.21, 'step': 0}, {'action': 'auto_sell', 'price': 431.29, 'step': 111}]\n",
      "Event 116 | PnL 0.0022 | trades [{'action': 'buy', 'price': 462.0, 'step': 0}, {'action': 'auto_sell', 'price': 484.75, 'step': 111}]\n",
      "Event 117 | PnL -0.0012 | trades [{'action': 'buy', 'price': 572.0, 'step': 0}, {'action': 'auto_sell', 'price': 560.41, 'step': 110}]\n",
      "Event 118 | PnL 0.0032 | trades [{'action': 'buy', 'price': 210.3, 'step': 0}, {'action': 'auto_sell', 'price': 242.91, 'step': 107}]\n",
      "Event 119 | PnL 0.0016 | trades [{'action': 'buy', 'price': 303.21, 'step': 0}, {'action': 'auto_sell', 'price': 319.57, 'step': 111}]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# --------------------------- main() -------------------------------\n",
    "####################################################################\n",
    "def main():\n",
    "    # ====== USER CONFIG ======\n",
    "    CSV_PATH = os.environ.get('CSV_PATH', 'earnings_events_data.csv')\n",
    "    TRANSACTION_COST = 0.0005\n",
    "    INITIAL_CASH = 10000\n",
    "    EPOCHS = 80\n",
    "    EVENTS_PER_EPOCH = 128\n",
    "    LR = 3e-4\n",
    "    SAVE_PATH = 'earnings_pg_agent.pth'\n",
    "    # =========================\n",
    "    \n",
    "    print(\"Loading CSV:\", CSV_PATH)\n",
    "    df, train_envs, test_envs = build_envs_from_csv(CSV_PATH, TRANSACTION_COST, INITIAL_CASH)\n",
    "    print(f\"Loaded {len(df):,} rows. Train events: {len(train_envs)}, Test events: {len(test_envs)}\")\n",
    "    \n",
    "    model, history = train(train_envs, test_envs=test_envs, epochs=EPOCHS, events_per_epoch=EVENTS_PER_EPOCH,\n",
    "                           lr=LR, save_path=SAVE_PATH)\n",
    "    \n",
    "    # quick eval summary on test set\n",
    "    if len(test_envs) > 0:\n",
    "        print(\"\\nSample evaluation on 10 test events (stochastic policy):\")\n",
    "        model.eval()\n",
    "        sample_ids = list(test_envs.keys())[:10]\n",
    "        with torch.no_grad():\n",
    "            for eid in sample_ids:\n",
    "                env = test_envs[eid]\n",
    "                s = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    a, _, _, _ = select_action(model, s)\n",
    "                    s, r, done, _ = env.step(a)\n",
    "                print(f\"Event {eid} | PnL {env.get_window_pnl():.4f} | trades {env.trades}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6f428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
